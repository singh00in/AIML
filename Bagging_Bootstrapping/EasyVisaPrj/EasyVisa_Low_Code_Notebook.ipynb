{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singh00in/AIML/blob/main/Bagging_Bootstrapping/EasyVisaPrj/EasyVisa_Low_Code_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZvo8CHcetWN",
      "metadata": {
        "id": "yZvo8CHcetWN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-shanghai",
      "metadata": {
        "id": "empty-shanghai"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n",
        "\n",
        "The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n",
        "\n",
        "OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment."
      ],
      "metadata": {
        "id": "wJmgAFK8_aCN"
      },
      "id": "wJmgAFK8_aCN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "3CWyKzKYAfEp"
      },
      "id": "3CWyKzKYAfEp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n",
        "\n",
        "The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n",
        "\n",
        "* Facilitate the process of visa approvals.\n",
        "* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status."
      ],
      "metadata": {
        "id": "cdq4cea5Akh8"
      },
      "id": "cdq4cea5Akh8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Description"
      ],
      "metadata": {
        "id": "LpNqj6EzAhxy"
      },
      "id": "LpNqj6EzAhxy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n",
        "\n",
        "* **case_id:** ID of each visa application\n",
        "* **continent:** Information of continent the employee\n",
        "* **education_of_employee:** Information of education of the employee\n",
        "* **has_job_experience:** Does the employee has any job experience? Y= Yes; N = No\n",
        "* **requires_job_training:** Does the employee require any job training? Y = Yes; N = No\n",
        "* **no_of_employees:** Number of employees in the employer's company\n",
        "* **yr_of_estab:** Year in which the employer's company was established\n",
        "* **region_of_employment:** Information of foreign worker's intended region of employment in the US.\n",
        "* **prevailing_wage:**  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\n",
        "* **unit_of_wage:** Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n",
        "* **full_time_position:** Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n",
        "* **case_status:**  Flag indicating if the Visa was certified or denied"
      ],
      "metadata": {
        "id": "uae5fMIyAnd-"
      },
      "id": "uae5fMIyAnd-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Please read the instructions carefully before starting the project.**"
      ],
      "metadata": {
        "id": "EWGlpDNkrTqA"
      },
      "id": "EWGlpDNkrTqA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks \"\\_\\_\\_\\_\\_\" are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every \"\\_\\_\\_\\_\\_\" blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations derived from the analysis in the presentation and submit the same.\n",
        "    * Any mathematical or computational details which are a graded part of the project can be included in the *Appendix* section of the presentation."
      ],
      "metadata": {
        "id": "oIAAXUOip1Fc"
      },
      "id": "oIAAXUOip1Fc"
    },
    {
      "cell_type": "markdown",
      "id": "Lm7obbsV_RUT",
      "metadata": {
        "id": "Lm7obbsV_RUT"
      },
      "source": [
        "## Installing and Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6IOeGuQTMQXd",
      "metadata": {
        "id": "6IOeGuQTMQXd"
      },
      "outputs": [],
      "source": [
        "# Installing the libraries with the specified version.\n",
        "!pip install numpy==1.25.2 pandas==1.5.3 scikit-learn==1.5.2 matplotlib==3.7.1 seaborn==0.13.1 xgboost==2.0.3 -q --user"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OS2VAv465IZa",
      "metadata": {
        "id": "OS2VAv465IZa"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "canadian-maple",
      "metadata": {
        "id": "canadian-maple"
      },
      "outputs": [],
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Library to split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "\n",
        "# libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "\n",
        "# Libraries different ensemble classifiers\n",
        "from sklearn.ensemble import (\n",
        "    BaggingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Libraries to get different metric scores\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# To tune different models\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# To ignore unnecessary warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thorough-passion",
      "metadata": {
        "id": "thorough-passion"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kqF861_Ch2ea",
      "metadata": {
        "id": "kqF861_Ch2ea"
      },
      "outputs": [],
      "source": [
        "# Uncomment and execute the code snippets below if the data is stored in Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alleged-teaching",
      "metadata": {
        "id": "alleged-teaching"
      },
      "outputs": [],
      "source": [
        "visa = pd.read_csv('_______') ## Complete the code to read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "falling-annual",
      "metadata": {
        "id": "falling-annual"
      },
      "outputs": [],
      "source": [
        "# Copying data to another variable to avoid making changes to the original data\n",
        "data = visa.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mq-1s9p-_aKl",
      "metadata": {
        "id": "mq-1s9p-_aKl"
      },
      "source": [
        "## Overview of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aboriginal-wrist",
      "metadata": {
        "id": "aboriginal-wrist"
      },
      "source": [
        "#### View the first and last 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "judicial-lease",
      "metadata": {
        "id": "judicial-lease"
      },
      "outputs": [],
      "source": [
        "data._____ ##  Complete the code to view top 5 rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-david",
      "metadata": {
        "id": "descending-david"
      },
      "outputs": [],
      "source": [
        "data._____ ##  Complete the code to view last 5 rows of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessory-camel",
      "metadata": {
        "id": "accessory-camel"
      },
      "source": [
        "#### Understand the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regulation-elder",
      "metadata": {
        "id": "regulation-elder"
      },
      "outputs": [],
      "source": [
        "data._______ ##  Complete the code to view dimensions of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "biblical-venice",
      "metadata": {
        "id": "biblical-venice"
      },
      "source": [
        "* The dataset has 25480 rows and 12 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assigned-berkeley",
      "metadata": {
        "id": "assigned-berkeley"
      },
      "source": [
        "#### Check the data types of the columns for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "judicial-institute",
      "metadata": {
        "id": "judicial-institute"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "going-validation",
      "metadata": {
        "id": "going-validation"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate values\n",
        "data._______ ##  Complete the code to check duplicate entries in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "standing-horizontal",
      "metadata": {
        "id": "standing-horizontal"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "american-venue",
      "metadata": {
        "id": "american-venue"
      },
      "source": [
        "#### Let's check the statistical summary of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "premium-wagner",
      "metadata": {
        "id": "premium-wagner"
      },
      "outputs": [],
      "source": [
        "data._______ ##  Complete the code to print the statistical summary of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "competent-timing",
      "metadata": {
        "id": "competent-timing"
      },
      "source": [
        "#### Fixing the negative values in number of employees columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "whole-saying",
      "metadata": {
        "id": "whole-saying"
      },
      "outputs": [],
      "source": [
        "data.loc[_______].shape ## Complete the code to check negative values in the employee column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prescription-bosnia",
      "metadata": {
        "id": "prescription-bosnia"
      },
      "outputs": [],
      "source": [
        "# taking the absolute values for number of employees\n",
        "data[\"no_of_employees\"] = abs(data[\"no_of_employees\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cutting-bookmark",
      "metadata": {
        "id": "cutting-bookmark"
      },
      "source": [
        "#### Let's check the count of each unique category in each of the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minute-helmet",
      "metadata": {
        "id": "minute-helmet"
      },
      "outputs": [],
      "source": [
        "# Making a list of all catrgorical variables\n",
        "cat_col = list(data.select_dtypes(\"object\").columns)\n",
        "\n",
        "# Printing number of count of each unique value in each column\n",
        "for column in cat_col:\n",
        "    print(data[column].value_counts())\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "occupied-classroom",
      "metadata": {
        "id": "occupied-classroom"
      },
      "outputs": [],
      "source": [
        "# checking the number of unique values\n",
        "data[\"case_id\"]._______ ## Complete the code to check unique values in the mentioned column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "existing-sociology",
      "metadata": {
        "id": "existing-sociology"
      },
      "outputs": [],
      "source": [
        "data.drop([\"________\"], axis=1, inplace=True) ## Complete the code to drop 'case_id' column from the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wooden-christian",
      "metadata": {
        "id": "wooden-christian"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superb-springfield",
      "metadata": {
        "id": "superb-springfield"
      },
      "outputs": [],
      "source": [
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R59VWoa5Wdbx",
      "metadata": {
        "id": "R59VWoa5Wdbx"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "editorial-command",
      "metadata": {
        "id": "editorial-command"
      },
      "source": [
        "#### Observations on education of employee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "express-harrison",
      "metadata": {
        "id": "express-harrison"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"education_of_employee\", perc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attempted-burlington",
      "metadata": {
        "id": "attempted-burlington"
      },
      "source": [
        "#### Observations on region of employment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "suited-tuner",
      "metadata": {
        "id": "suited-tuner"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data,\"_____\",perc=True)  ## Complete the code to create labeled_barplot for region of employment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forbidden-kidney",
      "metadata": {
        "id": "forbidden-kidney"
      },
      "source": [
        "#### Observations on job experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exterior-rings",
      "metadata": {
        "id": "exterior-rings"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data,\"_____\",perc=True) ## Complete the code to create labeled_barplot for job experience"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-surrey",
      "metadata": {
        "id": "stunning-surrey"
      },
      "source": [
        "#### Observations on case status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "potential-stroke",
      "metadata": {
        "id": "potential-stroke"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data,\"_____\",perc=True) ## Complete the code to create labeled_barplot for case status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equivalent-aging",
      "metadata": {
        "id": "equivalent-aging"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Check"
      ],
      "metadata": {
        "id": "ODJgdo_BwOFk"
      },
      "id": "ODJgdo_BwOFk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "social-wagner",
      "metadata": {
        "id": "social-wagner"
      },
      "outputs": [],
      "source": [
        "cols_list = data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(\n",
        "    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blond-convertible",
      "metadata": {
        "id": "blond-convertible"
      },
      "source": [
        "**Creating functions that will help us with further analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-recipient",
      "metadata": {
        "id": "adaptive-recipient"
      },
      "outputs": [],
      "source": [
        "### function to plot distributions wrt target\n",
        "\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "third-sheriff",
      "metadata": {
        "id": "third-sheriff"
      },
      "outputs": [],
      "source": [
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dressed-excuse",
      "metadata": {
        "id": "dressed-excuse"
      },
      "source": [
        "#### **Does higher education increase the chances of visa certification for well-paid jobs abroad?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "special-florist",
      "metadata": {
        "id": "special-florist"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"education_of_employee\", \"case_status\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attended-current",
      "metadata": {
        "id": "attended-current"
      },
      "source": [
        "#### **How does visa status vary across different continents?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alive-bloom",
      "metadata": {
        "id": "alive-bloom"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data,\"_____\",\"_____\") ## Complete the code to plot stacked barplot for continent and case status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "macro-decrease",
      "metadata": {
        "id": "macro-decrease"
      },
      "source": [
        "#### **Does having prior work experience influence the chances of visa certification for career opportunities abroad?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "divine-dimension",
      "metadata": {
        "id": "divine-dimension"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data,\"_____\",\"_____\") ## Complete the code to plot stacked barplot for case status and job experience"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changing-kansas",
      "metadata": {
        "id": "changing-kansas"
      },
      "source": [
        "#### **Is the prevailing wage consistent across all regions of the US?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-pride",
      "metadata": {
        "id": "dangerous-pride"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(data=data, x=\"_____\", y=\"_____\") ## Complete the code to create boxplot for region of employment and prevailing wage\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesser-bacteria",
      "metadata": {
        "id": "lesser-bacteria"
      },
      "source": [
        "#### **Does visa status vary with changes in the prevailing wage set to protect both local talent and foreign workers?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "perfect-haiti",
      "metadata": {
        "id": "perfect-haiti"
      },
      "outputs": [],
      "source": [
        "distribution_plot_wrt_target(data,\"_____\",\"_____\") ## Complete the code to find distribution of prevailing wage and case status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suspected-asthma",
      "metadata": {
        "id": "suspected-asthma"
      },
      "source": [
        "#### **Does the unit of prevailing wage (Hourly, Weekly, etc.) have any impact on the likelihood of visa application certification?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impossible-aquatic",
      "metadata": {
        "id": "impossible-aquatic"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data,\"_____\",\"_____\") ## Complete the code to plot stacked barplot for unit of wage and case status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBWlk20UBUAx",
      "metadata": {
        "id": "qBWlk20UBUAx"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "allied-association",
      "metadata": {
        "id": "allied-association"
      },
      "source": [
        "### Outlier Check\n",
        "\n",
        "- Let's check for outliers in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certified-complaint",
      "metadata": {
        "id": "certified-complaint"
      },
      "outputs": [],
      "source": [
        "# outlier detection using boxplot\n",
        "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, variable in enumerate(numeric_columns):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.boxplot(data[variable], whis=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-independence",
      "metadata": {
        "id": "flexible-independence"
      },
      "source": [
        "### Data Preparation for modeling\n",
        "\n",
        "- We want to predict which visa will be certified.\n",
        "- Before we proceed to build a model, we'll have to encode categorical features.\n",
        "- We'll split the data into train and test to be able to evaluate the model that we build on the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "genuine-evening",
      "metadata": {
        "id": "genuine-evening"
      },
      "outputs": [],
      "source": [
        "data[\"case_status\"] = data[\"case_status\"].apply(lambda x: 1 if x == \"Certified\" else 0)\n",
        "\n",
        "X = data.drop(['_____'],axis=1) ## Complete the code to drop case status from the data\n",
        "y = data[\"case_status\"]\n",
        "\n",
        "\n",
        "X = pd.get_dummies(X,drop_first=True)\n",
        "\n",
        "# Complete the code to split the dataset into train and valid with a ratio of 7:3\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=______, random_state=1, stratify=y\n",
        ")\n",
        "\n",
        "# # Complete the code to split the dataset into valid and test with a ratio of 9:1\n",
        "X_val,X_test,y_val,y_test = train_test_split(\n",
        "    X_val,y_val,test_size=_____,random_state=1,stratify=y_val\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empty-typing",
      "metadata": {
        "id": "empty-typing"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of Training set : \", X_train.shape)\n",
        "print(\"Shape of Validation set : \", X_val.shape)\n",
        "print(\"Shape of test set : \", X_test.shape)\n",
        "print(\"Percentage of classes in training set:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"Percentage of classes in validation set:\")\n",
        "print(y_val.value_counts(normalize=True))\n",
        "print(\"Percentage of classes in test set:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dr7q6-dbfiQB",
      "metadata": {
        "id": "dr7q6-dbfiQB"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rrlw9AVcqk37",
      "metadata": {
        "id": "rrlw9AVcqk37"
      },
      "source": [
        "### Model Evaluation Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Choose the primary metric to evaluate the model on\n",
        "- Elaborate on the rationale behind choosing the metric"
      ],
      "metadata": {
        "id": "p-08ovKQIx0D"
      },
      "id": "p-08ovKQIx0D"
    },
    {
      "cell_type": "markdown",
      "id": "capital-charlotte",
      "metadata": {
        "id": "capital-charlotte"
      },
      "source": [
        "First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
        "* The `model_performance_classification_sklearn` function will be used to check the model performance of models.\n",
        "* The `confusion_matrix_sklearn` function will be used to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mexican-database",
      "metadata": {
        "id": "mexican-database"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "\n",
        "\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recreational-topic",
      "metadata": {
        "id": "recreational-topic"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(predictors)\n",
        "    cm = confusion_matrix(target, y_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0QZZoxoDcoDm",
      "metadata": {
        "id": "0QZZoxoDcoDm"
      },
      "source": [
        "#### Defining scorer to be used for cross-validation and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fTqBmDuPm0hj",
      "metadata": {
        "id": "fTqBmDuPm0hj"
      },
      "outputs": [],
      "source": [
        "scorer = metrics.make_scorer(metrics._____) ## Complete the code to define the metric\n",
        "\n",
        "## Possible metrics are [recall_score,f1_score,accuracy_score,precision_score]\n",
        "## For example, metrics.precision_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XWoHuUpjbp0_",
      "metadata": {
        "id": "XWoHuUpjbp0_"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fI98GOV0pTY",
      "metadata": {
        "id": "4fI98GOV0pTY"
      },
      "source": [
        "### Model building with Original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-OTOG3o1bp0_",
      "metadata": {
        "id": "-OTOG3o1bp0_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n",
        "models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=_____, shuffle=True, random_state=1\n",
        "    )  # Complete the code to set the number of splits.\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring = scorer,cv=kfold\n",
        "    )\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(_____,_____) ## Complete the code to fit the model on original training data\n",
        "\n",
        "    # scores = accuracy_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is accuracy\n",
        "    # scores = recall_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is recall\n",
        "    # scores = precision_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is precision\n",
        "    # scores = f1_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is f1 score\n",
        "\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cm61fR_mbp0_",
      "metadata": {
        "id": "Cm61fR_mbp0_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.boxplot(results1)\n",
        "ax.set_xticklabels(names)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C91_6Swtbp1A",
      "metadata": {
        "id": "C91_6Swtbp1A"
      },
      "source": [
        "### Model Building with Oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o2dEk01hbp1A",
      "metadata": {
        "id": "o2dEk01hbp1A"
      },
      "outputs": [],
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "# Synthetic Minority Over Sampling Technique\n",
        "sm = SMOTE(sampling_strategy=1, k_neighbors=_____, random_state=1) ## Complete the code to set the k-nearest neighbors\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_over == 0)))\n",
        "\n",
        "\n",
        "print(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\n",
        "print(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZsOer36qbp1A",
      "metadata": {
        "id": "ZsOer36qbp1A"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n",
        "models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=_____, shuffle=True, random_state=1\n",
        "    )  ## Complete the code to set the number of splits\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train_over, y=y_train_over,scoring = scorer, cv=kfold\n",
        "    )\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(_____,_____) ## Complete the code to fit the model on oversampled training data\n",
        "\n",
        "    # scores = accuracy_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is accuracy\n",
        "    # scores = recall_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is recall\n",
        "    # scores = precision_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is precision\n",
        "    # scores = f1_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is f1 score\n",
        "\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kwUcBORjbp1A",
      "metadata": {
        "id": "kwUcBORjbp1A",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.boxplot(results1)\n",
        "ax.set_xticklabels(names)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fYLfDmHvbp1B",
      "metadata": {
        "id": "fYLfDmHvbp1B"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PHM-5g8Qbp1B",
      "metadata": {
        "id": "PHM-5g8Qbp1B"
      },
      "outputs": [],
      "source": [
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "\n",
        "print(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_un == 1)))\n",
        "print(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_un == 0)))\n",
        "\n",
        "\n",
        "print(\"After UnderSampling, the shape of train_X: {}\".format(X_train_un.shape))\n",
        "print(\"After UnderSampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rc5ndPMubp1B",
      "metadata": {
        "id": "Rc5ndPMubp1B",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "models.append((\"Random forest\", RandomForestClassifier(random_state=1)))\n",
        "models.append((\"GBM\", GradientBoostingClassifier(random_state=1)))\n",
        "models.append((\"Adaboost\", AdaBoostClassifier(random_state=1)))\n",
        "models.append((\"Xgboost\", XGBClassifier(random_state=1, eval_metric=\"logloss\")))\n",
        "models.append((\"dtree\", DecisionTreeClassifier(random_state=1)))\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=_____, shuffle=True, random_state=1\n",
        "    )  ## Complete the code to set the number of splits\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train_un, y=y_train_un,scoring = scorer, cv=kfold,n_jobs =-1\n",
        "    )\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(_____,_____) ## Complete the code to fit the model on undersampled training data\n",
        "\n",
        "    # scores = accuracy_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is accuracy\n",
        "    # scores = recall_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is recall\n",
        "    # scores = precision_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is precision\n",
        "    # scores = f1_score(y_val, model.predict(X_val)) ## uncomment this line in case the metric of choice is f1 score\n",
        "\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8HcDqaD2bp1B",
      "metadata": {
        "id": "8HcDqaD2bp1B",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.boxplot(results1)\n",
        "ax.set_xticklabels(names)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cg_OREBD1NOy",
      "metadata": {
        "id": "Cg_OREBD1NOy"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2FtmPS7Ubp1D",
      "metadata": {
        "id": "2FtmPS7Ubp1D"
      },
      "source": [
        "### Tuning AdaBoost using Oversampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best practices for hyperparameter tuning in AdaBoost:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "- Start with a specific number (50 is used in general) and increase in steps: 50, 75, 100, 125\n",
        "\n",
        "- Use fewer estimators (e.g., 50 to 100) if using complex base learners (like deeper decision trees)\n",
        "\n",
        "- Use more estimators (e.g., 100 to 150) when learning rate is low (e.g., 0.1 or lower)\n",
        "\n",
        "- Avoid very high values unless performance keeps improving on validation\n",
        "\n",
        "`learning_rate`:\n",
        "\n",
        "- Common values to try: 1.0, 0.5, 0.1, 0.01\n",
        "\n",
        "- Use 1.0 for faster training, suitable for fewer estimators\n",
        "\n",
        "- Use 0.1 or 0.01 when using more estimators to improve generalization\n",
        "\n",
        "- Avoid very small values (< 0.01) unless you plan to use many estimators (e.g., >500) and have sufficient data"
      ],
      "metadata": {
        "id": "rL58pS952AIO"
      },
      "id": "rL58pS952AIO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IPSlOlvkbp1D",
      "metadata": {
        "id": "IPSlOlvkbp1D"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# defining model\n",
        "model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [_____], ## Complete the code to set the number of estimators\n",
        "    \"learning_rate\": [_____], ## Complete the code to set the learning rate.\n",
        "    \"estimator\": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ]\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    n_jobs=-2,\n",
        "    scoring=scorer,\n",
        "    cv=_____, ## Complete the code to set the cv parameter\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(_____,_____) ## Complete the code to fit the model on oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "dZ1B3uLqpXxs"
      },
      "id": "dZ1B3uLqpXxs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xhrOotEVFuN8",
      "metadata": {
        "id": "xhrOotEVFuN8"
      },
      "outputs": [],
      "source": [
        "tuned_ada = randomized_cv.best_estimator_\n",
        "tuned_ada"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_ada.fit(X_train_over, y_train_over)"
      ],
      "metadata": {
        "id": "P-Dw3LnrpNeW"
      },
      "id": "P-Dw3LnrpNeW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGRRtFcsFxZD",
      "metadata": {
        "id": "kGRRtFcsFxZD"
      },
      "outputs": [],
      "source": [
        "tuned_ada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)\n",
        "tuned_ada_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10_dV5m4bp1D",
      "metadata": {
        "id": "10_dV5m4bp1D"
      },
      "outputs": [],
      "source": [
        "## Complete the code to check the model performance for validation data.\n",
        "tuned_ada_val_perf = model_performance_classification_sklearn(tuned_ada,_____,_____)\n",
        "tuned_ada_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dqj6dc38bp1E",
      "metadata": {
        "id": "Dqj6dc38bp1E"
      },
      "source": [
        "### Tuning Random forest using Undersampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best practices for hyperparameter tuning in Random Forest:**\n",
        "\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with a specific number (50 is used in general) and increase in steps: 50, 75, 100, 125\n",
        "* Higher values generally improve performance but increase training time\n",
        "* Use 100-150 for large datasets or when variance is high\n",
        "\n",
        "\n",
        "`min_samples_leaf`:\n",
        "\n",
        "* Try values like: 1, 2, 4, 5, 10\n",
        "* Higher values reduce model complexity and help prevent overfitting\n",
        "* Use 1–2 for low-bias models, higher (like 5 or 10) for more regularized models\n",
        "* Works well in noisy datasets to smooth predictions\n",
        "\n",
        "\n",
        "`max_features`:\n",
        "\n",
        "* Try values: `\"sqrt\"` (default for classification), `\"log2\"`, `None`, or float values (e.g., `0.3`, `0.5`)\n",
        "* `\"sqrt\"` balances between diversity and performance for classification tasks\n",
        "* Lower values (e.g., `0.3`) increase tree diversity, reducing overfitting\n",
        "* Higher values (closer to `1.0`) may capture more interactions but risk overfitting\n",
        "\n",
        "\n",
        "`max_samples` (for bootstrap sampling):\n",
        "\n",
        "* Try float values between `0.5` to `1.0` or fixed integers\n",
        "* Use `0.6–0.9` to introduce randomness and reduce overfitting\n",
        "* Smaller values increase diversity between trees, improving generalization"
      ],
      "metadata": {
        "id": "5wWUmnVj26bm"
      },
      "id": "5wWUmnVj26bm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9tLNoDYhbp1E",
      "metadata": {
        "id": "9tLNoDYhbp1E"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# defining model\n",
        "model = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [_____], ## Complete the code to set the number of estimators.\n",
        "    \"min_samples_leaf\": [_____], ## Complete the code to set the minimum number of samples in the leaf node.\n",
        "    \"max_features\": [_____], ## Complete the code to set the maximum number of features.\n",
        "    \"max_samples\": [_____], ## Complete the code to set the maximum number of samples.\n",
        "}\n",
        "\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    n_jobs=-2,\n",
        "    scoring=scorer,\n",
        "    cv=_____, ## Complete the code to set the cv parameter\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(_____,_____) ## Complete the code to fit the model on undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "VHHncDy-zSS_"
      },
      "id": "VHHncDy-zSS_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_rf = randomized_cv.best_estimator_\n",
        "tuned_rf"
      ],
      "metadata": {
        "id": "IZXZ2zi7zSxF"
      },
      "id": "IZXZ2zi7zSxF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_rf.fit(X_train_over, y_train_over)"
      ],
      "metadata": {
        "id": "tBZ4SHoyzWzv"
      },
      "id": "tBZ4SHoyzWzv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bAAD5mzybp1E",
      "metadata": {
        "id": "bAAD5mzybp1E"
      },
      "outputs": [],
      "source": [
        "tuned_rf_train_perf = model_performance_classification_sklearn(\n",
        "    tuned_rf, X_train_un, y_train_un\n",
        ")\n",
        "tuned_rf_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WTtYpzO-bp1E",
      "metadata": {
        "id": "WTtYpzO-bp1E",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## Complete the code to print the model performance on the validation data.\n",
        "tuned_rf_val_perf = model_performance_classification_sklearn(tuned_rf,_____,_____)\n",
        "tuned_rf_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0uLE_tPV2Roq",
      "metadata": {
        "id": "0uLE_tPV2Roq"
      },
      "source": [
        "### Tuning with Gradient boosting with Oversampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best practices for hyperparameter tuning in Gradient Boosting:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with 100 (default) and increase: 100, 125, 150, 175\n",
        "* Typically, higher values lead to better performance, but they also increase training time\n",
        "* Use 150-250 for larger datasets or complex problems\n",
        "* Monitor validation performance to avoid overfitting, as too many estimators can degrade generalization\n",
        "\n",
        "\n",
        "`learning_rate`:\n",
        "\n",
        "* Common values to try: 0.1, 0.05, 0.01, 0.005\n",
        "* Use lower values (e.g., 0.01 or 0.005) if you are using many estimators (e.g., > 200)\n",
        "* Higher learning rates (e.g., 0.1) can be used with fewer estimators for faster convergence\n",
        "* Always balance the learning rate with `n_estimators` to prevent overfitting or underfitting\n",
        "\n",
        "\n",
        "`subsample`:\n",
        "\n",
        "* Common values: 0.7, 0.8, 0.9, 1.0\n",
        "* Use a value between `0.7` and `0.9` for improved generalization by introducing randomness\n",
        "* `1.0` uses the full dataset for each boosting round, potentially leading to overfitting\n",
        "* Reducing `subsample` can help reduce overfitting, especially in smaller datasets\n",
        "\n",
        "\n",
        "`max_features`:\n",
        "\n",
        "* Common values: `\"sqrt\"`, `\"log2\"`, or float (e.g., `0.3`, `0.5`)\n",
        "* `\"sqrt\"` (default) works well for classification tasks\n",
        "* Lower values (e.g., `0.3`) help reduce overfitting by limiting the number of features considered at each split"
      ],
      "metadata": {
        "id": "-bCkPdz53SoN"
      },
      "id": "-bCkPdz53SoN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ikBgsbjn2RHu",
      "metadata": {
        "id": "ikBgsbjn2RHu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# defining model\n",
        "model = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "## Complete the code to define the hyper parameters.\n",
        "param_grid={\n",
        "    \"n_estimators\": [_____], ## Complete the code to set the number of estimators.\n",
        "    \"learning_rate\": [_____], ## Complete the code to set the learning rate.\n",
        "    \"subsample\":[_____], ## Complete the code to set the value for subsample.\n",
        "    \"max_features\":[_____] ## Complete the code to set the value for max_features.\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    n_jobs=-2,\n",
        "    scoring=scorer,\n",
        "    cv=_____, ## Complete the code to set the cv parameter\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "OqSwX8BKzmdS"
      },
      "id": "OqSwX8BKzmdS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_gbm = randomized_cv.best_estimator_\n",
        "tuned_gbm"
      ],
      "metadata": {
        "id": "N0OLUk3mznDp"
      },
      "id": "N0OLUk3mznDp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXkAXXum2Q0-",
      "metadata": {
        "id": "fXkAXXum2Q0-"
      },
      "outputs": [],
      "source": [
        "tuned_gbm_train_perf = model_performance_classification_sklearn(\n",
        "    tuned_gbm, X_train_over, y_train_over\n",
        ")\n",
        "tuned_gbm_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZF3y7Jl12Qi-",
      "metadata": {
        "id": "ZF3y7Jl12Qi-"
      },
      "outputs": [],
      "source": [
        "## Complete the code to print the model performance on the validation data.\n",
        "tuned_gbm_val_perf = model_performance_classification_sklearn(tuned_gbm,_____,_____)\n",
        "tuned_gbm_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VbrQHwrKbp1C",
      "metadata": {
        "id": "VbrQHwrKbp1C"
      },
      "source": [
        "### [OPTIONAL] Tuning XGBoost using Oversampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best practices for hyperparameter tuning in XGBoost:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with 50 and increase in steps: 50,75,100,125.\n",
        "* Use more estimators (e.g., 150-250) when using lower learning rates\n",
        "* Monitor validation performance\n",
        "* High values improve learning but increase training time\n",
        "\n",
        "`subsample`:\n",
        "\n",
        "* Common values: 0.5, 0.7, 0.8, 1.0\n",
        "* Use `0.7–0.9` to introduce randomness and reduce overfitting\n",
        "* `1.0` uses the full dataset in each boosting round; may overfit on small datasets\n",
        "* Values < 0.5 are rarely useful unless dataset is very large\n",
        "\n",
        "`gamma`:\n",
        "\n",
        "* Try values: 0 (default), 1, 3, 5, 8\n",
        "* Controls minimum loss reduction needed for a split\n",
        "* Higher values make the algorithm more conservative (i.e., fewer splits)\n",
        "* Use values > 0 to regularize and reduce overfitting, especially on noisy data\n",
        "\n",
        "\n",
        "`colsample_bytree`:\n",
        "\n",
        "* Try values: 0.3, 0.5, 0.7, 1.0\n",
        "* Fraction of features sampled per tree\n",
        "* Lower values (e.g., 0.3 or 0.5) increase randomness and improve generalization\n",
        "* Use `1.0` when you want all features considered for every tree\n",
        "\n",
        "\n",
        "`colsample_bylevel`:\n",
        "\n",
        "* Try values: 0.3, 0.5, 0.7, 1.0\n",
        "* Fraction of features sampled at each tree level (i.e., per split depth)\n",
        "* Lower values help in regularization and reducing overfitting\n",
        "* Often used in combination with `colsample_bytree` for fine control over feature sampling"
      ],
      "metadata": {
        "id": "CUfQUd9BohjX"
      },
      "id": "CUfQUd9BohjX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8e6iVzbp1C",
      "metadata": {
        "id": "ec8e6iVzbp1C"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# defining model\n",
        "model = XGBClassifier(random_state=1,eval_metric='logloss')\n",
        "\n",
        "## Complete the code to define the hyperparameters\n",
        "param_grid={\n",
        "    'n_estimators':[_____], ## Complete the code to set the number of estimators.\n",
        "    'subsample':[_____], ## Complete the code to set the subsample.\n",
        "    'gamma':[_____], ## Complete the code to set the gamma.\n",
        "    'colsample_bytree':[_____], ## Complete the code to set the value for colsample_bytree.\n",
        "    'colsample_bylevel':[_____], ## Complete the code to set the value for colsample_bylevel.\n",
        "}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    n_jobs=-2,\n",
        "    scoring=scorer,\n",
        "    cv=_____, ## Complete the code to set the cv parameter\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over)## Complete the code to fit the model on oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ],
      "metadata": {
        "id": "mKpnJBvqzvtz"
      },
      "id": "mKpnJBvqzvtz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_xgb = randomized_cv.best_estimator_\n",
        "tuned_xgb"
      ],
      "metadata": {
        "id": "qPKgPFZdzyWc"
      },
      "id": "qPKgPFZdzyWc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDzHR8Pwbp1D",
      "metadata": {
        "id": "hDzHR8Pwbp1D"
      },
      "outputs": [],
      "source": [
        "tuned_xgb_train_perf = model_performance_classification_sklearn(\n",
        "    tuned_xgb, X_train_over, y_train_over\n",
        ")\n",
        "tuned_xgb_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jiDRsOrMbp1D",
      "metadata": {
        "id": "jiDRsOrMbp1D",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "## Complete the code to print the model performance on the validation data.\n",
        "tuned_xgb_val_perf = model_performance_classification_sklearn(tuned_xgb,_____,_____)\n",
        "tuned_xgb_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fyp5xd91Z-W",
      "metadata": {
        "id": "4fyp5xd91Z-W"
      },
      "source": [
        "**We have now tuned all the models, let's compare the performance of all tuned models and see which one is the best.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D9JNnpxa4jau",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Model Performance Summary and Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TF1WfZZHbp1E",
      "metadata": {
        "id": "TF1WfZZHbp1E"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        tuned_gbm_train_perf.T,\n",
        "        # tuned_xgb_train_perf.T, ## uncomment this line if XGBoost model was tuned\n",
        "        tuned_ada_train_perf.T,\n",
        "        tuned_rf_train_perf.T,\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Gradient Boosting tuned with oversampled data\",\n",
        "    # \"XGBoost tuned with oversampled data\", ## uncomment this line if XGBoost model was tuned\n",
        "    \"AdaBoost tuned with oversampled data\",\n",
        "    \"Random forest tuned with undersampled data\",\n",
        "]\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0FgTXpsXbp1F",
      "metadata": {
        "id": "0FgTXpsXbp1F"
      },
      "outputs": [],
      "source": [
        "# validation performance comparison\n",
        "\n",
        "models_val_comp_df = pd.concat(\n",
        "    [\n",
        "        tuned_gbm_val_perf.T,\n",
        "        # tuned_xgb_val_perf.T, ## uncomment this line if XGBoost model was tuned\n",
        "        tuned_ada_val_perf.T,\n",
        "        tuned_rf_val_perf.T,\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_val_comp_df.columns = [\n",
        "    \"Gradient Boosting tuned with oversampled data\",\n",
        "    # \"XGBoost tuned with oversampled data\", ## uncomment this line if XGBoost model was tuned\n",
        "    \"AdaBoost tuned with oversampled data\",\n",
        "    \"Random forest tuned with undersampled data\",\n",
        "]\n",
        "print(\"Validation performance comparison:\")\n",
        "models_val_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting the final model by uncommenting the line corresponding to the final model chosen\n",
        "\n",
        "# final_model = tuned_gbm ## uncomment this line if the final model chosen is Gradient Boosting\n",
        "# final_model = tuned_xgb ## uncomment this line if the final model chosen is XGBoost\n",
        "# final_model = tuned_ada ## uncomment this line if the final model chosen is AdaBoost\n",
        "# final_model = tuned_rf ## uncomment this line if the final model chosen is Random Forest"
      ],
      "metadata": {
        "id": "hgA3_lOI3R-q"
      },
      "id": "hgA3_lOI3R-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qrWw_SWBkJtV",
      "metadata": {
        "id": "qrWw_SWBkJtV"
      },
      "outputs": [],
      "source": [
        "test = model_performance_classification_sklearn(final_model, X_test, y_test)\n",
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tlY4dfMuVBUz",
      "metadata": {
        "id": "tlY4dfMuVBUz"
      },
      "outputs": [],
      "source": [
        "feature_names = X_train.columns\n",
        "importances = final_model.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "congressional-knock",
      "metadata": {
        "id": "congressional-knock"
      },
      "source": [
        "## Actionable Insights and Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This section can be attempted directly in the presentation."
      ],
      "metadata": {
        "id": "AOX0jFfDrI4m"
      },
      "id": "AOX0jFfDrI4m"
    },
    {
      "cell_type": "markdown",
      "id": "Y2HdXLmSJi8K",
      "metadata": {
        "id": "Y2HdXLmSJi8K"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yZvo8CHcetWN",
        "empty-shanghai",
        "3CWyKzKYAfEp",
        "LpNqj6EzAhxy",
        "EWGlpDNkrTqA",
        "Lm7obbsV_RUT",
        "thorough-passion",
        "mq-1s9p-_aKl",
        "aboriginal-wrist",
        "accessory-camel",
        "assigned-berkeley",
        "standing-horizontal",
        "american-venue",
        "competent-timing",
        "cutting-bookmark",
        "wooden-christian",
        "editorial-command",
        "attempted-burlington",
        "forbidden-kidney",
        "stunning-surrey",
        "equivalent-aging",
        "ODJgdo_BwOFk",
        "qBWlk20UBUAx",
        "allied-association",
        "flexible-independence",
        "dr7q6-dbfiQB",
        "rrlw9AVcqk37",
        "4fI98GOV0pTY",
        "C91_6Swtbp1A",
        "fYLfDmHvbp1B",
        "Cg_OREBD1NOy",
        "2FtmPS7Ubp1D",
        "Dqj6dc38bp1E",
        "0uLE_tPV2Roq",
        "VbrQHwrKbp1C",
        "D9JNnpxa4jau",
        "congressional-knock"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}